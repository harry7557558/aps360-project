{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/tux/school/360/360-proj/data_processing.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tux/school/360/360-proj/data_processing.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# pip install torch \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tux/school/360/360-proj/data_processing.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tux/school/360/360-proj/data_processing.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader, Dataset\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tux/school/360/360-proj/data_processing.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mT\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# pip install torch \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "# https://www.kaggle.com/datasets/yashchoudhary/realsr-v3/data\n",
    "# https://github.com/ngchc/CameraSR (City100)\n",
    "# https://github.com/xiezw5/Component-Divide-and-Conquer-for-Real-World-Image-Super-Resolution?tab=readme-ov-file\n",
    "\n",
    "# First, pick the 2000 image pairs and have two folders (train and test) and in each are 2 more folders (hr and lr). \n",
    "# .   \n",
    "# ├── train\n",
    "# │   ├── HR\n",
    "# │   └── LR\n",
    "# └── test\n",
    "#     ├── HR\n",
    "#     └── LR\n",
    "# Pre-split already, 80-20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATASET_IMAGES_DIR = \"./images\" # Path to your directory where the images/ folder is \n",
    "CROPPED_DATASET_IMAGES_DIR = \"./cropped_images\" # Path to an empty directory for the cropped images\n",
    "\n",
    "os.makedirs(\"{}/train/HR\".format(CROPPED_DATASET_IMAGES_DIR), exist_ok=True)\n",
    "os.makedirs(\"{}/test/HR\".format(CROPPED_DATASET_IMAGES_DIR), exist_ok=True)\n",
    "os.makedirs(\"{}/train/LR\".format(CROPPED_DATASET_IMAGES_DIR), exist_ok=True)\n",
    "os.makedirs(\"{}/test/LR\".format(CROPPED_DATASET_IMAGES_DIR), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly crop the images and store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "from os import listdir\n",
    "\n",
    "# Random crop transformation to apply\n",
    "transform = T.Compose([\n",
    "        T.RandomCrop(128)\n",
    "])\t\t\n",
    "\n",
    "directories = [\"test\", \"train\"]\n",
    "for directory in directories:\n",
    "\t# Deal with transforming both the LR and HR images at the same time in order \n",
    "\t# to transform them the same way\n",
    "\tHR_dir = sorted(os.listdir(\"{}/{}/HR\".format(DATASET_IMAGES_DIR, directory)))\n",
    "\tLR_dir = sorted(os.listdir(\"{}/{}/LR\".format(DATASET_IMAGES_DIR, directory)))\n",
    "\tfor imageIndex,img in enumerate(LR_dir):\n",
    "\n",
    "\t\timgLR = Image.open(\"{}/{}/LR/\".format(DATASET_IMAGES_DIR, directory)+LR_dir[imageIndex])\n",
    "\t\timgHR = Image.open(\"{}/{}/HR/\".format(DATASET_IMAGES_DIR, directory)+HR_dir[imageIndex])\n",
    "\n",
    "\t\t# Save the state and reload the rng state so that the same transformation is applied\n",
    "\t\t# to both images\n",
    "\t\tstate = torch.get_rng_state()\n",
    "\t\timgLR = transform(imgLR)\n",
    "\t\ttorch.set_rng_state(state)\n",
    "\t\timgHR = transform(imgHR)\n",
    "\n",
    "\t\t# Save the images in the cropped dataset folder\n",
    "\t\timgLR.save(\"{}/{}/LR/\".format(CROPPED_DATASET_IMAGES_DIR, directory)+LR_dir[imageIndex])\n",
    "\t\timgHR.save(\"{}/{}/HR/\".format(CROPPED_DATASET_IMAGES_DIR, directory)+HR_dir[imageIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset class\n",
    "# Code used from: https://discuss.pytorch.org/t/how-can-i-read-a-image-pair-at-the-same-time/23151\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, lr_paths_dir, hr_paths_dir):\n",
    "        self.lr_paths = lr_paths # List of image paths: ['./train/LR/image_1.bmp', './train/input/image_2.bmp', ...]\n",
    "        self.hr_paths = hr_paths\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        lr_img = Image.open(self.lr_paths[index])\n",
    "        hr_img = Image.open(self.hr_paths[index])\n",
    "        return lr_img, hr_img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lr_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MyDataset(\n",
    "    sorted(os.listdir(\"{}/{}/LR\".format(CROPPED_DATASET_IMAGES_DIR, \"train\"))), sorted(os.listdir(\"{}/{}/HR\".format(CROPPED_DATASET_IMAGES_DIR, \"train\"))))\n",
    "\n",
    "test_set = MyDataset(\n",
    "    sorted(os.listdir(\"{}/{}/LR\".format(CROPPED_DATASET_IMAGES_DIR, \"test\"))), sorted(os.listdir(\"{}/{}/HR\".format(CROPPED_DATASET_IMAGES_DIR, \"test\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and validation indices to split the train_set\n",
    "dataset_size = len(train_set)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "split = int(len(indices) * 0.8) #split at 80%\n",
    "\n",
    "train_indices, val_indices = indices[:split], indices[split:]\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "# Create a Dataloader for each train, val, and test\n",
    "train_dl = DataLoader(train_set, batch_size=batch_size, \n",
    "\t\t\t\t\t\t\t\t\t\t\tnum_workers=1, sampler=train_sampler)\n",
    "val_dl = DataLoader(train_set, batch_size=batch_size,\n",
    "                      num_workers=1, sampler=val_sampler)\n",
    "test_dl = DataLoader(test_set, batch_size=batch_size,\n",
    "                      num_workers=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
