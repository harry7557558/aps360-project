{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/tux/school/360/360-proj/data_processing.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tux/school/360/360-proj/data_processing.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# pip install torch \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tux/school/360/360-proj/data_processing.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tux/school/360/360-proj/data_processing.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader, Dataset\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tux/school/360/360-proj/data_processing.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mT\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# pip install torch \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "# https://www.kaggle.com/datasets/yashchoudhary/realsr-v3/data\n",
    "# https://github.com/ngchc/CameraSR (City100)\n",
    "# https://github.com/xiezw5/Component-Divide-and-Conquer-for-Real-World-Image-Super-Resolution?tab=readme-ov-file\n",
    "\n",
    "# First, pick the 2000 image pairs and have two folders (train and test) and in each are 2 more folders (hr and lr). TODO:\n",
    "# .   \n",
    "# ├── train\n",
    "# │   ├── HR\n",
    "# │   └── LR\n",
    "# └── test\n",
    "#     ├── HR\n",
    "#     └── LR\n",
    "# Pre-split already, 80 20 split?\n",
    "# Pre-crop as well TODO:\n",
    "\n",
    "# transform = T.Compose([\n",
    "#         # T.ToPILImage(), It's already a PIL image I think\n",
    "#         T.CenterCrop(128),\n",
    "#         T.Resize(image_size),\n",
    "#         T.ToTensor()])\n",
    "# This is what will be stored in the github?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset class\n",
    "# Code used from: https://discuss.pytorch.org/t/how-can-i-read-a-image-pair-at-the-same-time/23151\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, lr_paths, hr_paths):\n",
    "        self.lr_paths = lr_paths # List of image paths: ['./train/LR/image_1.bmp', './train/input/image_2.bmp', ...] TODO: How to generate this list of paths??\n",
    "        self.hr_paths = hr_paths\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        lr_img = Image.open(self.lr_paths[index])\n",
    "        hr_img = Image.open(self.hr_paths[index])\n",
    "\n",
    "\t\t\t\t# TODO: transform it to a tensor? \n",
    "\t\t\t\t# # transform = T.Compose([T.ToTensor()])\n",
    "\t\t\t\t# lr_img = self.transform(lr_img)\n",
    "\t\t\t\t# hr_img = self.transform(hr_img)\n",
    "\n",
    "        \n",
    "            \n",
    "        return lr_img, hr_img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lr_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MyDataset(\n",
    "    image1_paths, image2_paths, transform=transforms.ToTensor())\n",
    "\n",
    "test_set = MyDataset(\n",
    "    image1_paths, image2_paths, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and validation indices to split the train_set\n",
    "dataset_size = len(train_set)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "split = int(len(indices) * 0.8) #split at 80%\n",
    "\n",
    "train_indices, val_indices = indices[:split], indices[split:]\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "# Create a Dataloader for each train, val, and test\n",
    "train_dl = DataLoader(trainset, batch_size=batch_size, \n",
    "\t\t\t\t\t\t\t\t\t\t\tnum_workers=1, sampler=train_sampler)\n",
    "val_dl = DataLoader(trainset, batch_size=batch_size,\n",
    "                      num_workers=1, sampler=val_sampler)\n",
    "test_dl = DataLoader(test_set, batch_size=batch_size,\n",
    "                      num_workers=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
